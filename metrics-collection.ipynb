{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jira import JIRA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "\n",
    "import json\n",
    "\n",
    "with open('jira.json') as json_data_file:\n",
    "    data = json.load(json_data_file)\n",
    "    username = data['auth']['username']\n",
    "    password = data['auth']['password']\n",
    "    bugqueryadd = data['bugqueryadd']\n",
    "    epicqueryadd = data['epicqueryadd']\n",
    "    storyqueryadd = data['storyqueryadd']\n",
    "    domain = data['domain']\n",
    "    columns = data['columns']\n",
    "    fields = data['fields']\n",
    "    outfile = data['outfile']\n",
    "\n",
    "if not domain:\n",
    "    domain = raw_input(\"Jira Domain (e.g https://XXX:PPP/jira): \")\n",
    "\n",
    "if not username:\n",
    "    username = raw_input(\"Username: \")\n",
    "\n",
    "if not password:\n",
    "    password = getpass.getpass(\"Password: \")\n",
    "\n",
    "if not columns:\n",
    "    columns = raw_input(\"Columns (List of colums): \")\n",
    "\n",
    "if not fields:\n",
    "    fields = raw_input(\"Fields (List of JQL fields): \")\n",
    "\n",
    "if not bugqueryadd:\n",
    "    bugqueryadd = raw_input(\"List of fixversions (no quotes, commas allowed):\")\n",
    "    bugqueryadd = 'fixversion in (' + bugqueryadd + ')'\n",
    "\n",
    "if not epicqueryadd:\n",
    "    epicqueryadd = raw_input(\"List of fixversions (no quotes, commas allowed):\")\n",
    "    epicqueryadd = 'fixversion in (' + epicqueryadd + ')'\n",
    "\n",
    "if not storyqueryadd:\n",
    "    storyqueryadd = raw_input(\"List of fixversions (no quotes, commas allowed):\")\n",
    "    storyqueryadd = 'fixversion in (' + storyqueryadd + ')'\n",
    "    \n",
    "def get_jira_client(domain, username, password):\n",
    "    options = {'server': domain}\n",
    "    return JIRA(options, basic_auth=(username, password))\n",
    "\n",
    "def print_jira_issue(issue):\n",
    "    print (issue['key'], \":\", issue['fields']['summary'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jira = get_jira_client(domain, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epics = jira.search_issues('type=epic and ' + epicqueryadd, json_result=True, maxResults=1000, fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = jira.search_issues('type=story and ' + storyqueryadd, json_result=True, maxResults=1000, fields = fields, expand='changelog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs = jira.search_issues('type=bug and ' + bugqueryadd, json_result=True, maxResults=1000, fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep the stories and epics dataframes\n",
    "#fix the column names\n",
    "#extract comment data \n",
    "#extract all the history from stories and build all the workflow fields\n",
    "\n",
    "for issue in stories['issues']:\n",
    "    #merge the textual fields of comments, summary\n",
    "    alltext = [comment['body'] for comment in issue['fields']['comment']['comments']]\n",
    "    if (issue['fields']['summary'] != None):\n",
    "        alltext.append(issue['fields']['summary'])\n",
    "    if (issue['fields']['description'] != None):\n",
    "        alltext.append(issue['fields']['description'])\n",
    "    try:\n",
    "        issue['fields']['textinfo'] = ' '.join(alltext)\n",
    "    except TypeError:\n",
    "        print(alltext)\n",
    "\n",
    "    #for stories only, record the important parts of change log as separate columns\n",
    "    changelog = issue['changelog']\n",
    "    for history in changelog['histories']:\n",
    "        for item in history['items']:\n",
    "            if item['field'] == 'status':\n",
    "                issue['fields'][item['toString'] + ' ' + 'Set To Date'] = history['created']\n",
    "                issue['fields'][item['toString'] + ' ' + 'Set By'] = history['author']['name']\n",
    "\n",
    "for issue in epics['issues']:\n",
    "    alltext = [comment['body'] for comment in issue['fields']['comment']['comments']]\n",
    "    alltext.append(issue['fields']['summary'])\n",
    "    #alltext.append(issue['fields']['description'])\n",
    "    issue['fields']['textinfo'] = ' '.join(alltext)\n",
    "\n",
    "epic_list = []\n",
    "for epic in epics['issues']:\n",
    "    epic['fields']['key'] = epic['key']\n",
    "    epic_list.append(epic['fields'])\n",
    "\n",
    "epics_df = pd.DataFrame(epic_list)\n",
    "\n",
    "story_list = []\n",
    "for story in stories['issues']:\n",
    "    story['fields']['key'] = story['key']\n",
    "    story_list.append(story['fields'])\n",
    "\n",
    "stories_df = pd.DataFrame(story_list)\n",
    "\n",
    "#replacement of custom field's by their names is only done inside the dataframe\n",
    "# Fetch all fields\n",
    "allfields=jira.fields()\n",
    "# Make a map from field name -> field id\n",
    "nameMap = {field['name']:field['id'] for field in allfields}\n",
    "idMap = {field['id']:field['name'] for field in allfields}\n",
    "\n",
    "for column in epics_df.columns:\n",
    "    if ('custom' in column):\n",
    "        epics_df.rename(columns={column: idMap[column]}, inplace=True)\n",
    "\n",
    "for column in stories_df.columns:\n",
    "    if ('custom' in column):\n",
    "        stories_df.rename(columns={column: idMap[column]}, inplace=True)\n",
    "\n",
    "stories_df['Team'] = stories_df['Team'].dropna().apply(lambda x: x[0].get('value') if (type(x) == list) else None)\n",
    "stories_df['status'] = stories_df['status'].dropna().apply(lambda x: x.get('name'))\n",
    "stories_df['reporter'] = stories_df['reporter'].dropna().apply(lambda x: x.get('name'))\n",
    "stories_df['fixVersions'] = stories_df['fixVersions'].dropna().apply(lambda x: x[0].get('name')if (type(x) == dict) else None)\n",
    "stories_df['Platform'] = stories_df['Platform'].dropna().apply(lambda x: x[0].get('value'))\n",
    "\n",
    "#Change the string time fields into the python datetime structures\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "stories_df['Approval Set To Date'] = pd.to_datetime(stories_df['Approval Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['Closed Set To Date'] = pd.to_datetime(stories_df['Closed Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['Code Review Set To Date'] = pd.to_datetime(stories_df['Code Review Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['In Analysis Set To Date'] = pd.to_datetime(stories_df['In Analysis Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['In Progress Set To Date'] = pd.to_datetime(stories_df['In Progress Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['In UI/UX Set To Date'] = pd.to_datetime(stories_df['In UI/UX Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['Open Set To Date'] = pd.to_datetime(stories_df['Open Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['Ready for Estimation Set To Date'] = pd.to_datetime(stories_df['Ready for Estimation Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['Testing Set To Date'] = pd.to_datetime(stories_df['Testing Set To Date'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the sprint information from the sprints field and create a separate sprints-issue dataframe\n",
    "#this is only possible once we have the stories dataframe\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "#Takes a list of sprints of the form:\n",
    "#['com.atlassian.greenhopper.service.sprint.Sprint@1b7eb58a[id=519,rapidViewId=219,state=CLOSED,name=Knight Riders Sprint 2018 - 22,startDate=2018-05-23T21:16:06.149+05:30,endDate=2018-06-05T19:44:00.000+05:30,completeDate=2018-06-06T20:45:27.547+05:30,sequence=519]',\n",
    "# 'com.atlassian.greenhopper.service.sprint.Sprint@2a28663d[id=542,rapidViewId=219,state=ACTIVE,name=Knight Riders Sprint 2018-23,startDate=2018-06-06T22:14:10.412+05:30,endDate=2018-06-19T20:42:00.000+05:30,completeDate=<null>,sequence=542]']\n",
    "# and returns one list with a dictionary object for each sprint located. The object also contains the issue key\n",
    "# the other is \n",
    "# we return a dictionary\n",
    "def getSprintInfo(issueKey, sprint):\n",
    "    #locate the part in square braces\n",
    "    start = sprint.find('[') + 1\n",
    "    end = sprint.find(']', start)\n",
    "    dict_sprint = dict(x.split('=') for x in sprint[start:end].split(','))\n",
    "    dict_sprint['issue_key'] = issueKey\n",
    "    return dict_sprint\n",
    "\n",
    "#we return a list of dictionaries, where each dictionary is a sprint paired with the issue.\n",
    "def getSprints (issueKey, sprints):\n",
    "    if type(sprints) == list:\n",
    "        return [getSprintInfo(issueKey, sprint) for sprint in sprints]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "x1 = []\n",
    "for index, row in stories_df.iterrows():\n",
    "    x1 = x1 + (getSprints(row['key'], row['Sprint']))\n",
    "\n",
    "#x1 = scope_df.apply(lambda x: getSprints(x['key_story'], x['Sprint_story']), axis=1).dropna()\n",
    "#x1\n",
    "\n",
    "#y = reduce((lambda x, y: x + y), x1)\n",
    "\n",
    "sprints_df =  pd.DataFrame(x1)\n",
    "sprints_df['endDate'] = pd.to_datetime(sprints_df['endDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "sprints_df['startDate'] = pd.to_datetime(sprints_df['startDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "sprints_df['completeDate'] = pd.to_datetime(sprints_df['completeDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep up the bugs dataframe\n",
    "\n",
    "bugs_list = []\n",
    "for bug in bugs['issues']:\n",
    "    bug['fields']['key'] = bug['key']\n",
    "    for issuelink in bug['fields']['issuelinks']:\n",
    "        try:\n",
    "            if ((issuelink['outwardIssue']['fields']['issuetype']['name'] == 'Story') and \n",
    "            ((issuelink['type']['outward'] == 'associated with') or \n",
    "             (issuelink['type']['outward'] == 'relates to'))):\n",
    "                bug['fields']['linkKey'] = issuelink['outwardIssue']['key']\n",
    "                bug['fields']['linktype'] = issuelink['type']['outward']\n",
    "        except:\n",
    "            #print(issuelink)\n",
    "            if ((issuelink['inwardIssue']['fields']['issuetype']['name'] == 'Story') and \n",
    "            ((issuelink['type']['inward'] == 'associated with') or \n",
    "             (issuelink['type']['inward'] == 'relates to'))):\n",
    "                bug['fields']['linkKey'] = issuelink['inwardIssue']['key']\n",
    "                bug['fields']['linktype'] = issuelink['type']['inward']\n",
    "    #add each bug to bug list after updating the fields\n",
    "    bugs_list.append(bug['fields'])\n",
    "        \n",
    "bugs_df = pd.DataFrame(bugs_list)\n",
    "\n",
    "for column in bugs_df.columns:\n",
    "    if ('custom' in column):\n",
    "        bugs_df.rename(columns={column: idMap[column]}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first merge - create the epics and stories merge\n",
    "scope_df = pd.merge(epics_df, stories_df, how='right', on=None, left_on='key', right_on='Epic Link',\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_epic', '_story'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "\n",
    "#insert a column for jira link\n",
    "scope_df['story_link'] = '=HYPERLINK(\"' + domain + '/browse/' + scope_df['key_story'] + '\",\"' + scope_df['key_story'] + '\")'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the sprints with the epics and stories dataframe and we can then drop the duplicate issue_key field.\n",
    "\n",
    "sprintsWithStoriesAndEpics_df = pd.merge(scope_df, sprints_df, how='left', on=None, left_on='key_story', right_on='issue_key',\n",
    "         left_index=False, right_index=False, \n",
    "         suffixes=('_story', '_sprint'),\n",
    "         copy=True, indicator=False,\n",
    "         validate=None).drop(columns = ['issue_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After combining sprints with the stories and epics we can now filter out records where the \n",
    "#end date of the sprint was prior to our window of interest\n",
    "\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['endDate'] > datetime(2018, 4, 3)]\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['endDate'] < datetime(2018, 7, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets do some basic statistics\n",
    "#get the number of unique stories - note that these stories are duplicated because they are part of multiple sprints\n",
    "#in some cases.\n",
    "# also this is the stories that were worked on and not necessariy finished. They were simply inside the sprints\n",
    "sprintsWithStoriesAndEpics_df['key_story'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintsWithStoriesAndEpics_df['key_epic'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets eliminate the stories which are not closed yet and then count the unique stories.\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['status_story'] == 'Closed']\n",
    "sprintsWithStoriesAndEpics_df['key_story'].unique().shape\n",
    "\n",
    "#Note if there is a difference from previous count to check if all stories were closed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of stories each team worked on, number of points each team covered, number of bugs\n",
    "#each team fixed, number of features that were worked on.\n",
    "\n",
    "#first add up the number of sprints a story is in\n",
    "#sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df.drop_duplicates(subset = 'key_story')\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'key_story', 'Story Points', 'name']].copy()\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.groupby(['Team_story']).agg({'key_story':['count'], 'Story Points':['sum'], 'name':['nunique']})\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy.columns\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['average velocity'] = sprintsWithStoriesAndEpics_dfCopy['Story Points']['sum']/sprintsWithStoriesAndEpics_dfCopy['name']['nunique']\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the spillover stories per team\n",
    "\n",
    "#first add up the number of sprints a story is in\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'key_story', 'name', 'startDate', 'Open Set To Date']].copy()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] = (sprintsWithStoriesAndEpics_dfCopy['startDate'] - sprintsWithStoriesAndEpics_dfCopy['Open Set To Date']).dt.days \n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] = sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] > -2\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy.duplicated(keep=False, subset='key_story')].to_excel(writer, index=False, sheet_name='Spillover Stories', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'name', 'startDate', 'Open Set To Date', 'sprintLeadTime', 'sprintCommitment'])\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['sprintCommitment']].sort_values(by='key_story')\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy= sprintsWithStoriesAndEpics_dfCopy.drop(columns = ['startDate', 'Open Set To Date', 'sprintCommitment', 'sprintLeadTime'])\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.groupby(['Team_story', 'key_story']).agg(['count'])\n",
    "\n",
    "#reset index since we need to do another groupby\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.reset_index()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['spillover sprint count'] = sprintsWithStoriesAndEpics_dfCopy['name']['count']\n",
    "sprintsWithStoriesAndEpics_dfCopy= sprintsWithStoriesAndEpics_dfCopy.drop(columns = ['name'])\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.groupby(['Team_story', 'spillover sprint count']).agg(['count'])\n",
    "sprintsWithStoriesAndEpics_dfCopy.groupby(level=0).apply(max)\n",
    "sprintsWithStoriesAndEpics_dfCopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index since we need to do another groupby\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.reset_index()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['story count'] = sprintsWithStoriesAndEpics_dfCopy['key_story']['count']\n",
    "sprintsWithStoriesAndEpics_dfCopy= sprintsWithStoriesAndEpics_dfCopy.drop(columns = ['key_story'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets calculate the weighted average\n",
    "sprintsWithStoriesAndEpics_dfCopy.groupby(['Team_story']).apply(lambda g: np.average(g['spillover sprint count'], weights=g['story count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join bugs with sprints to determine how many bugs were attached to sprints and hence part of the relevant period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find total number of bugs created within sprints and compare with bugs created in total within the period. Note we need \n",
    "#to focus on created bugs and not the ones which were fixed.\n",
    "\n",
    "#also need to compare bugs found during regression with the sprint bugs\n",
    "\n",
    "#bugs resolved but not closed\n",
    "\n",
    "#qa and sprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the bugs with the stories dataframe\n",
    "#find the number of bugs for each story point\n",
    "\n",
    "storiesWithBugs_df = pd.merge(bugs_df, stories_df, how='right', on=None, left_on='linkKey', right_on='key',\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_bug', '_story'), copy=True, indicator=False,\n",
    "         validate=None)\n",
    "\n",
    "storiesWithBugs_df = storiesWithBugs_df[['Team_story', 'key_story', 'key_bug', 'Code Review Set By', 'reporter_story',\n",
    "                                         'Story Points_story']].copy().dropna()\n",
    "storiesWithBugs_df = storiesWithBugs_df.groupby(['Team_story', 'key_story', 'Code Review Set By', 'reporter_story', 'Story Points_story']).agg(['count'])\n",
    "storiesWithBugs_df = storiesWithBugs_df.reset_index()\n",
    "storiesWithBugs_df['bugs per story point'] = storiesWithBugs_df['key_bug']['count']/storiesWithBugs_df['Story Points_story']\n",
    "\n",
    "#write out the source data onto disk\n",
    "storiesWithBugs_df.to_excel(writer, index=True, sheet_name='Bugs per Story Point', freeze_panes=(1,0))\n",
    "\n",
    "storiesWithBugs_df = storiesWithBugs_df.drop(columns = ['Story Points_story', 'key_story', 'key_bug'])\n",
    "storiesWithBugs_df = storiesWithBugs_df.groupby(['Team_story', 'Code Review Set By', 'reporter_story']).agg(['mean'])\n",
    "storiesWithBugs_df = storiesWithBugs_df.reset_index()\n",
    "\n",
    "\n",
    "storiesWithBugs_df['avg bugs per story point'] = storiesWithBugs_df['bugs per story point']['mean']\n",
    "storiesWithBugs_df= storiesWithBugs_df.drop(columns = ['bugs per story point'])\n",
    "\n",
    "storiesWithBugs_df = storiesWithBugs_df.sort_values(by='avg bugs per story point', ascending=False)\n",
    "storiesWithBugs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the stories which were inserted in sprints after sprints started\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'startDate', 'Open Set To Date', 'reporter_story', 'Story Points', 'key_story', 'name']].copy()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] = (sprintsWithStoriesAndEpics_dfCopy['startDate'] - sprintsWithStoriesAndEpics_dfCopy['Open Set To Date']).dt.days \n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] = sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] > -2\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] != True].sort_values(by='key_story')\n",
    "#sprintsWithStoriesAndEpics_dfCopy['key_story'].unique().size\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "sprintsWithStoriesAndEpics_dfCopy.to_excel(writer, index=False, sheet_name='Late Commitments to Sprint', freeze_panes=(1,0), columns=['Team_story', 'startDate', 'Open Set To Date', 'reporter_story', 'Story Points', 'key_story', 'name', 'sprintLeadTime', 'sprintCommitment'])\n",
    "\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy.duplicated(subset='key_story')]['key_story'].unique().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop unnecessary columns before we do stats\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.drop(columns=['startDate', 'Open Set To Date', 'Story Points', 'name', 'sprintCommitment'])\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.groupby(['reporter_story', 'Team_story']).agg({'sprintLeadTime':['mean'], 'key_story':['count']})\n",
    "\n",
    "#we must filter the noise\n",
    "#sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.reset_index()\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['key_story']['count'] > 5]\n",
    "sprintsWithStoriesAndEpics_dfCopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changes to description of story after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of bugs found post sprints are over that need to be fixed in release\n",
    "#bug creation date > end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of issues left in Testing and Testing lead time inside sprint\n",
    "#find the issues that are still in Testing before the end of their sprint. Only include issues that were committed \n",
    "#to in the beginning of the sprint.\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'startDate', 'endDate', 'Testing Set To Date', 'Approval Set To Date', 'Approval Set By', 'Open Set To Date', 'key_story']].copy()\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.dropna()\n",
    "sprintsWithStoriesAndEpics_dfCopy['Testing Lead Time'] = (sprintsWithStoriesAndEpics_dfCopy['endDate'] - sprintsWithStoriesAndEpics_dfCopy['Testing Set To Date']).dt.days\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['Testing Lead Time'] >= 2] \n",
    "sprintsWithStoriesAndEpics_dfCopy['Ready for Approval Delay'] = (sprintsWithStoriesAndEpics_dfCopy['Approval Set To Date'] - sprintsWithStoriesAndEpics_dfCopy['endDate']).dt.days\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['Ready for Approval Delay'] >= 2]\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] = (sprintsWithStoriesAndEpics_dfCopy['startDate'] - sprintsWithStoriesAndEpics_dfCopy['Open Set To Date']).dt.days \n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] = sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] > -2\n",
    "#sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['sprintCommitment']].sort_values(by='key_story')\n",
    "sprintsWithStoriesAndEpics_dfCopy['Testing Time'] = sprintsWithStoriesAndEpics_dfCopy['Testing Lead Time'] + sprintsWithStoriesAndEpics_dfCopy['Ready for Approval Delay']\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "sprintsWithStoriesAndEpics_dfCopy.to_excel(writer, index=False, sheet_name='Testing Spillovers in Sprints', freeze_panes=(1,0), columns=['Team_story', 'startDate', 'endDate', 'Testing Set To Date', 'Approval Set To Date', 'Approval Set By', 'Open Set To Date', 'key_story', 'Testing Lead Time', 'Ready for Approval Delay', 'Testing Time'])\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.sort_values(by='key_story').drop(columns = ['Ready for Approval Delay', 'startDate', 'endDate', 'Testing Set To Date', 'Approval Set To Date', 'Open Set To Date', 'sprintLeadTime', 'sprintCommitment'])\n",
    "\n",
    "\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['key_story'].unique().size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintsWithStoriesAndEpics_dfCopy.groupby(['Approval Set By', 'Team_story']).agg({'key_story':['count'], 'Testing Lead Time':['mean'], 'Testing Time':['mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of issues left in Approval and Approval lead time inside sprint\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'startDate', 'endDate', 'Approval Set To Date', 'Approval Set By', 'Closed Set By', 'Open Set To Date', 'Closed Set To Date', 'key_story']].copy()\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy.dropna()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['Approval Lead Time'] = (sprintsWithStoriesAndEpics_dfCopy['endDate'] - sprintsWithStoriesAndEpics_dfCopy['Approval Set To Date']).dt.days\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['Approval Lead Time'] >= 2] \n",
    "sprintsWithStoriesAndEpics_dfCopy['Close Delay'] = (sprintsWithStoriesAndEpics_dfCopy['Closed Set To Date'] - sprintsWithStoriesAndEpics_dfCopy['endDate']).dt.days\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['Close Delay'] >= 2]\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] = (sprintsWithStoriesAndEpics_dfCopy['startDate'] - sprintsWithStoriesAndEpics_dfCopy['Open Set To Date']).dt.days \n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] = sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] > -2\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_dfCopy[sprintsWithStoriesAndEpics_dfCopy['sprintCommitment']].sort_values(by='key_story')\n",
    "sprintsWithStoriesAndEpics_dfCopy['Approval Time'] = sprintsWithStoriesAndEpics_dfCopy['Approval Lead Time'] + sprintsWithStoriesAndEpics_dfCopy['Close Delay']\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "sprintsWithStoriesAndEpics_dfCopy.to_excel(writer, index=False, sheet_name='Approval Spillovers in Sprints', freeze_panes=(1,0), columns=['Team_story', 'startDate', 'endDate', 'Approval Set To Date', 'Approval Set By', 'Closed Set By', 'Open Set To Date', 'Closed Set To Date', 'key_story', 'Approval Lead Time', 'Close Delay', 'Approval Time'])\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprintsWithStoriesAndEpics_dfCopy.groupby(['Closed Set By', 'Team_story']).agg({'key_story':['count'], 'Approval Lead Time':['mean'], 'Approval Time':['mean']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a list of strings\n",
    "#scope_df['textinfo'] = scope_df['textinfo_story'] + scope_df['textinfo_epic']\n",
    "scope_df['textinfo'] = scope_df['textinfo_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_df['Invalid AC'] = scope_df['textinfo'].str.contains('Acceptance|AC', case = False, regex = True) == False\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "scope_df[scope_df['Invalid AC']].to_excel(writer, index=False, sheet_name='Invalid AC', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'reporter_story'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_ac_df = scope_df[['reporter_story', 'Invalid AC']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce statistics for valid/invalid AC\n",
    "invalid_ac_df.groupby(['reporter_story']).sum().sort_values(by=['Invalid AC'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.DataFrame(scope_df, columns = ['Epic Name', 'textinfo'])\n",
    "dataset1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#create document vectors\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors_ds1 = vectorizer.fit_transform(dataset1.textinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split this into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predictors = vectors_ds1\n",
    "targets = dataset1['Epic Name']\n",
    "\n",
    "pred_train, pred_test, tar_train, tar_test  =   train_test_split(predictors, targets, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "sklearn.metrics.confusion_matrix(tar_test,predictions)\n",
    "sklearn.metrics.accuracy_score(tar_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_ds1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
