{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Set up Jira Client</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following packages must be installed after anaconda is installed. They are commented off here.\n",
    "#!pip install jira\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install xslwriter\n",
    "#!pip install json\n",
    "#!pip intsall datetime\n",
    "#!pip install functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jira import JIRA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "json_data_file = domain = domain = username = password = None\n",
    "cpath = \"./jira.json\"\n",
    "\n",
    "while not json_data_file:\n",
    "    try:\n",
    "        json_data_file = open(cpath)\n",
    "    except FileNotFoundError:\n",
    "        cpath = input('Directory Path of jira.json: ')\n",
    "        cpath = cpath + '/jira.json'\n",
    "\n",
    "data = json.load(json_data_file)\n",
    "username = data['auth']['username']\n",
    "password = data['auth']['password']\n",
    "bugqueryadd = data['bugqueryadd']\n",
    "epicqueryadd = data['epicqueryadd']\n",
    "storyqueryadd = data['storyqueryadd']\n",
    "domain = data['domain']\n",
    "columns = data['columns']\n",
    "fields = data['fields']\n",
    "outfile = data['outfile']\n",
    "        \n",
    "#if not domain:\n",
    "#    domain = input(\"Jira Domain (e.g https://XXX:PPP/jira): \")\n",
    "\n",
    "#Only username and password will be accepted outside of the file\n",
    "if not username:\n",
    "    username = input(\"Username: \")\n",
    "\n",
    "if not password:\n",
    "    password = getpass.getpass(\"Password: \")\n",
    "   \n",
    "def get_jira_client(domain, username, password):\n",
    "    options = {'server': domain}\n",
    "    return JIRA(options, basic_auth=(username, password))\n",
    "    \n",
    "writer = pd.ExcelWriter(outfile)\n",
    "jira = get_jira_client(domain, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Important dates/labels that set the baseline for this run\n",
    "qtrStart = '2018-07-04'\n",
    "qtrEnd = '2018-09-25'\n",
    "qtrStartDate = pd.to_datetime(qtrStart, format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "qtrEndDate = pd.to_datetime(qtrEnd, format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "relp = 'R18'\n",
    "reln = 'R19'\n",
    "\n",
    "#for relp:\n",
    "#last deadline for inserting stories\n",
    "releaseStoryDeadline = datetime(2018, 7, 24)\n",
    "#monitoring deadline for removing stories\n",
    "releaseStoryRemovalMonitoringStart = datetime(2018, 6, 12)\n",
    "releaseStoryRemovalMonitoringEnd = datetime(2018, 9, 11)\n",
    "\n",
    "bins = [datetime(2018, 6, 19), datetime(2018, 7, 3), datetime(2018, 7, 17), \n",
    "        datetime(2018, 7, 31), datetime(2018, 8, 14), datetime(2018, 8, 28), datetime(2018, 9, 11), \n",
    "        datetime(2018, 9, 25), datetime(2018, 10, 9)]\n",
    "binLabels = ['reg-sprint-24', 'r18-sprint-25', 'r18-sprint-26', \n",
    "             'r18-sprint-27', 'reg-sprint-28', \n",
    "          'reg-sprint-29', 'r19-sprint-30', 'r19-sprint-31']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Load Stories, Epics and Bugs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epics = jira.search_issues('type=epic and ' + epicqueryadd, json_result=True, maxResults=20000, fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = jira.search_issues('type=story and ' + storyqueryadd, json_result=True, maxResults=20000, fields = fields, expand='changelog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Set up the Dataframes for Stories and Epics</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep the stories and epics dataframes\n",
    "#fix the column names\n",
    "#extract comment data \n",
    "#extract all the history from stories and build all the workflow fields\n",
    "\n",
    "for issue in stories['issues']:\n",
    "    #merge the textual fields of comments, summary\n",
    "    alltext = [comment['body'] for comment in issue['fields']['comment']['comments']]\n",
    "    if (issue['fields']['summary'] != None):\n",
    "        alltext.append(issue['fields']['summary'])\n",
    "    if (issue['fields']['description'] != None):\n",
    "        alltext.append(issue['fields']['description'])\n",
    "    try:\n",
    "        issue['fields']['textinfo'] = ' '.join(alltext)\n",
    "    except TypeError:\n",
    "        print(alltext)\n",
    "\n",
    "    #for stories only, record the important parts of change log as separate columns\n",
    "    \n",
    "    issue['fields']['Open Set By'] = []\n",
    "    issue['fields']['Approval Set By'] = []\n",
    "    issue['fields']['Closed Set By'] = []\n",
    "    issue['fields']['Code Review Set By'] = []\n",
    "    issue['fields']['In Analysis Set By'] = []\n",
    "    issue['fields']['In Progress Set By'] = []\n",
    "    issue['fields']['In UI/UX Set By'] = []\n",
    "    issue['fields']['Ready for Estimation Set By'] = []\n",
    "    issue['fields']['Testing Set By'] = []\n",
    "    issue['fields']['Resolved Set By'] = []\n",
    "    issue['fields']['Reopened Set By'] = []\n",
    "    \n",
    "    changelog = issue['changelog']\n",
    "    for history in changelog['histories']:\n",
    "        for item in history['items']:\n",
    "            #print (item['field'])\n",
    "            if (item['field'] == 'Fix Version') and (item['fromString'] == relp):\n",
    "                #a story was moved out of the current fix version?\n",
    "                issue['fields']['FixVersion Change Date'] = pd.to_datetime(history['created'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "                #print(issue['key'], ' fix version changed from ', item['fromString'], ' to ', item['toString'])\n",
    "            if item['field'] == 'status':\n",
    "                #need to ensure if there are multiple times a certain status is updated, we capture it\n",
    "                #the first or last time based on the specific status.\n",
    "                timestamp = pd.to_datetime(history['created'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "                event = item['toString'] + ' ' + 'Set By'\n",
    "                author = history['author']['name']\n",
    "                issue['fields'][event].append((author, timestamp))\n",
    "                #issue['fields'][item['toString'] + ' ' + 'Set To Date'] = history['created']\n",
    "                #issue['fields'][item['toString'] + ' ' + 'Set By'] = history['author']['name']\n",
    "    issue['fields']['Open Set By'] = min(issue['fields']['Open Set By'], key = lambda t: t[1]) if issue['fields']['Open Set By'] else None\n",
    "    issue['fields']['Approval Set By'] = max(issue['fields']['Approval Set By'], key = lambda t: t[1]) if issue['fields']['Approval Set By'] else None\n",
    "    issue['fields']['Closed Set By'] = max(issue['fields']['Closed Set By'], key = lambda t: t[1]) if issue['fields']['Closed Set By'] else None\n",
    "    issue['fields']['Code Review Set By'] = min(issue['fields']['Code Review Set By'], key = lambda t: t[1]) if issue['fields']['Code Review Set By'] else None\n",
    "    issue['fields']['In Analysis Set By'] = min(issue['fields']['In Analysis Set By'], key = lambda t: t[1]) if issue['fields']['In Analysis Set By'] else None\n",
    "    issue['fields']['In Progress Set By'] = min(issue['fields']['In Progress Set By'], key = lambda t: t[1]) if issue['fields']['In Progress Set By'] else None\n",
    "    issue['fields']['In UI/UX Set By'] = min(issue['fields']['In UI/UX Set By'], key = lambda t: t[1]) if issue['fields']['In UI/UX Set By'] else None\n",
    "    issue['fields']['Ready for Estimation Set By'] = min(issue['fields']['Ready for Estimation Set By'], key = lambda t: t[1]) if issue['fields']['Ready for Estimation Set By'] else None\n",
    "    issue['fields']['Testing Set By'] = min(issue['fields']['Testing Set By'], key = lambda t: t[1]) if issue['fields']['Testing Set By'] else None\n",
    "    issue['fields']['Resolved Set By'] = min(issue['fields']['Resolved Set By'], key = lambda t: t[1]) if issue['fields']['Resolved Set By'] else None\n",
    "    issue['fields']['Reopened Set By'] = min(issue['fields']['Reopened Set By'], key = lambda t: t[1]) if issue['fields']['Reopened Set By'] else None\n",
    "    \n",
    "    issue['fields']['Open Set To Date'] = issue['fields']['Open Set By'][1] if issue['fields']['Open Set By'] else None\n",
    "    issue['fields']['Open Set By'] = issue['fields']['Open Set By'][0] if issue['fields']['Open Set By'] else None\n",
    "    \n",
    "    issue['fields']['Approval Set To Date'] = issue['fields']['Approval Set By'][1] if issue['fields']['Approval Set By'] else None\n",
    "    issue['fields']['Approval Set By'] = issue['fields']['Approval Set By'][0] if issue['fields']['Approval Set By'] else None\n",
    "    \n",
    "    issue['fields']['Closed Set To Date'] = issue['fields']['Closed Set By'][1] if issue['fields']['Closed Set By'] else None\n",
    "    issue['fields']['Closed Set By'] = issue['fields']['Closed Set By'][0] if issue['fields']['Closed Set By'] else None\n",
    "    \n",
    "    issue['fields']['Code Review Set To Date'] = issue['fields']['Code Review Set By'][1] if issue['fields']['Code Review Set By'] else None\n",
    "    issue['fields']['Code Review Set By'] = issue['fields']['Code Review Set By'][0] if issue['fields']['Code Review Set By'] else None\n",
    "    \n",
    "    issue['fields']['In Analysis Set To Date'] = issue['fields']['In Analysis Set By'][1] if issue['fields']['In Analysis Set By'] else None\n",
    "    issue['fields']['In Analysis Set By'] = issue['fields']['In Analysis Set By'][0] if issue['fields']['In Analysis Set By'] else None\n",
    "    \n",
    "    issue['fields']['In Progress Set To Date'] = issue['fields']['In Progress Set By'][1] if issue['fields']['In Progress Set By'] else None\n",
    "    issue['fields']['In Progress Set By'] = issue['fields']['In Progress Set By'][0] if issue['fields']['In Progress Set By'] else None\n",
    "    \n",
    "    issue['fields']['In UI/UX Set To Date'] = issue['fields']['In UI/UX Set By'][1] if issue['fields']['In UI/UX Set By'] else None\n",
    "    issue['fields']['In UI/UX Set By'] = issue['fields']['In UI/UX Set By'][0] if issue['fields']['In UI/UX Set By'] else None\n",
    "    \n",
    "    issue['fields']['Ready for Estimation Set To Date'] = issue['fields']['Ready for Estimation Set By'][1] if issue['fields']['Ready for Estimation Set By'] else None\n",
    "    issue['fields']['Ready for Estimation Set By'] = issue['fields']['Ready for Estimation Set By'][0] if issue['fields']['Ready for Estimation Set By'] else None\n",
    "    \n",
    "    issue['fields']['Testing Set To Date'] = issue['fields']['Testing Set By'][1] if issue['fields']['Testing Set By'] else None\n",
    "    issue['fields']['Testing Set By'] = issue['fields']['Testing Set By'][0] if issue['fields']['Testing Set By'] else None\n",
    "    \n",
    "    issue['fields']['Resolved Set To Date'] = issue['fields']['Resolved Set By'][1] if issue['fields']['Resolved Set By'] else None\n",
    "    issue['fields']['Resolved Set By'] = issue['fields']['Resolved Set By'][0] if issue['fields']['Resolved Set By'] else None\n",
    "    \n",
    "    issue['fields']['Reopened Set To Date'] = issue['fields']['Reopened Set By'][1] if issue['fields']['Reopened Set By'] else None\n",
    "    issue['fields']['Reopened Set By'] = issue['fields']['Reopened Set By'][0] if issue['fields']['Reopened Set By'] else None\n",
    "    \n",
    "    \n",
    "for issue in epics['issues']:\n",
    "    alltext = [comment['body'] for comment in issue['fields']['comment']['comments']]\n",
    "    alltext.append(issue['fields']['summary'])\n",
    "    #alltext.append(issue['fields']['description'])\n",
    "    issue['fields']['textinfo'] = ' '.join(alltext)\n",
    "\n",
    "epic_list = []\n",
    "for epic in epics['issues']:\n",
    "    epic['fields']['key'] = epic['key']\n",
    "    epic_list.append(epic['fields'])\n",
    "\n",
    "epics_df = pd.DataFrame(epic_list)\n",
    "\n",
    "story_list = []\n",
    "for story in stories['issues']:\n",
    "    story['fields']['key'] = story['key']\n",
    "    story_list.append(story['fields'])\n",
    "\n",
    "stories_df = pd.DataFrame(story_list)\n",
    "\n",
    "#replacement of custom field's by their names is only done inside the dataframe\n",
    "# Fetch all fields\n",
    "allfields=jira.fields()\n",
    "# Make a map from field name -> field id\n",
    "nameMap = {field['name']:field['id'] for field in allfields}\n",
    "idMap = {field['id']:field['name'] for field in allfields}\n",
    "\n",
    "for column in epics_df.columns:\n",
    "    if ('custom' in column):\n",
    "        epics_df.rename(columns={column: idMap[column]}, inplace=True)\n",
    "\n",
    "for column in stories_df.columns:\n",
    "    if ('custom' in column):\n",
    "        stories_df.rename(columns={column: idMap[column]}, inplace=True)\n",
    "\n",
    "stories_df['Team'] = stories_df['Team'].apply(lambda x: x[0].get('value') if (type(x) == list) else None)\n",
    "stories_df['status'] = stories_df['status'].apply(lambda x: x.get('name'))\n",
    "stories_df['reporter'] = stories_df['reporter'].apply(lambda x: x.get('name'))\n",
    "stories_df['fixVersions'] = stories_df['fixVersions'].apply(lambda x: x[0]['name'] if ((type(x) == list) and x and (type(x[0]) == dict)) else None)\n",
    "stories_df['Platform'] = stories_df['Platform'].apply(lambda x: x[0].get('value'))\n",
    "stories_df['created'] = pd.to_datetime(stories_df['created'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "stories_df['resolution'] = stories_df['resolution'].apply(lambda x: x['name'] if type(x) == dict else None)\n",
    "\n",
    "\n",
    "#insert a column for jira link\n",
    "stories_df['story_link'] = '=HYPERLINK(\"' + domain + '/browse/' + stories_df['key'] + '\",\"' + stories_df['key'] + '\")'\n",
    "\n",
    "#eliminate stories that are marked not needed\n",
    "stories_df = stories_df[stories_df['resolution'] != 'Not Needed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Set up the Dataframes for Sprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the sprint information from the sprints field and create a separate sprints-issue dataframe\n",
    "#this is only possible once we have the stories dataframe\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "#Takes a list of sprints of the form:\n",
    "#['com.atlassian.greenhopper.service.sprint.Sprint@1b7eb58a[id=519,rapidViewId=219,state=CLOSED,name=Knight Riders Sprint 2018 - 22,startDate=2018-05-23T21:16:06.149+05:30,endDate=2018-06-05T19:44:00.000+05:30,completeDate=2018-06-06T20:45:27.547+05:30,sequence=519]',\n",
    "# 'com.atlassian.greenhopper.service.sprint.Sprint@2a28663d[id=542,rapidViewId=219,state=ACTIVE,name=Knight Riders Sprint 2018-23,startDate=2018-06-06T22:14:10.412+05:30,endDate=2018-06-19T20:42:00.000+05:30,completeDate=<null>,sequence=542]']\n",
    "# and returns one list with a dictionary object for each sprint located. The object also contains the issue key\n",
    "# the other is \n",
    "# we return a dictionary\n",
    "def getSprintInfo(issueKey, sprint):\n",
    "    #locate the part in square braces\n",
    "    start = sprint.find('[') + 1\n",
    "    end = sprint.find(']', start)\n",
    "    dict_sprint = dict(x.split('=') for x in sprint[start:end].split(','))\n",
    "    dict_sprint['issue_key'] = issueKey\n",
    "    return dict_sprint\n",
    "\n",
    "#we return a list of dictionaries, where each dictionary is a sprint paired with the issue.\n",
    "def getSprints (issueKey, sprints):\n",
    "    if type(sprints) == list:\n",
    "        return [getSprintInfo(issueKey, sprint) for sprint in sprints]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "x1 = []\n",
    "for index, row in stories_df.iterrows():\n",
    "    x1 = x1 + (getSprints(row['key'], row['Sprint']))\n",
    "\n",
    "sprints_df =  pd.DataFrame(x1)\n",
    "sprints_df['endDate'] = pd.to_datetime(sprints_df['endDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "sprints_df['startDate'] = pd.to_datetime(sprints_df['startDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')\n",
    "sprints_df['completeDate'] = pd.to_datetime(sprints_df['completeDate'], format='%Y-%m-%dT%H:%M:%S.%f', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Story and Epics distribution at the top level</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Basic statistics before we start separating\n",
    "\n",
    "print('No. Epics: ', epics_df['key'].unique().size)\n",
    "print('No. Stories: ', stories_df['key'].unique().size)\n",
    "print('No of stories without linked epics: ', sum(pd.isnull(stories_df['Epic Link'])))\n",
    "print ('Stories not Closed: ', stories_df[stories_df['status'] != 'Closed']['key'].unique().size)\n",
    "print ('Stories without a fixVersion: ', stories_df[stories_df['fixVersions'] == None]['key'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets find the distribution of status amongst all the fixVersions\n",
    "\n",
    "storiesFixVersionsStatus_df = stories_df[['fixVersions', 'status', 'key']].copy()\n",
    "storiesFixVersionsStatus_df.groupby(['fixVersions', 'status']).agg(['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories_df[pd.notnull(stories_df['FixVersion Change Date']) & (stories_df['fixVersions'] != relp)][['key', 'Team', 'reporter', 'summary', 'FixVersion Change Date', 'fixVersions']].sort_values('Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Stories that potentially have wrong fixVersion</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stories that in Code Review/Testing or Approval in reln need to be flagged\n",
    "df = stories_df[((stories_df['fixVersions'].isin([reln, 'Backlog']) | pd.isnull(stories_df['fixVersions'])) & \n",
    "                        (stories_df['status'].isin(['Code Review', 'In Progress', 'Approval', 'Closed'])) )]\n",
    "df[['key', 'status', 'fixVersions', 'summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove the stories which we do not care about - not in relp or reln\n",
    "stories_df = stories_df[((stories_df['fixVersions'] == relp) | (stories_df['fixVersions'] == reln))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did we change fixversion recently of a story that was potentially required in current release?\n",
    "#Go through history and make a list of tickets that have had their fixVersion either set to relp or set to something \n",
    "#other than relp. So the From or To could be relp and we just need a date threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For stories that are closed, lets find the time it took for us to go through each state completely, \n",
    "#the points of the story, the number of sprints it took, the team the story is in.\n",
    "#We are ignoring the Reopen workflow.\n",
    "\n",
    "stories_df['Analysis Duration'] = (stories_df['Ready for Estimation Set To Date'] - stories_df['created']).dt.days\n",
    "stories_df['Dev Duration'] = (stories_df['Testing Set To Date'] - stories_df['Open Set To Date']).dt.days\n",
    "stories_df['QA Duration'] = (stories_df['Approval Set To Date'] - stories_df['Testing Set To Date']).dt.days\n",
    "stories_df['Approval Duration'] = (stories_df['Closed Set To Date'] - stories_df['Approval Set To Date']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = stories_df[stories_df['status'] == 'Closed']\n",
    "df = stories_df\n",
    "df = df[(df['Analysis Duration'] > 60) | (df['Dev Duration'] > 7) | (df['QA Duration'] > 2) | (df['Approval Duration'] > 1)]\n",
    "df[['key', 'fixVersions', 'status', 'Team', 'Analysis Duration', 'Dev Duration', 'QA Duration', 'Approval Duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcuate the age of the stories in the last state it is in\n",
    "now = datetime.now() + pd.Timedelta('010:30:00')\n",
    "stories_df['Age In Days'] = stories_df.apply(lambda x: (now - x[x['status'] + ' Set To Date']).days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Merge Stories, Epics and Sprints</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first merge - create the epics and stories merge\n",
    "scope_df = pd.merge(epics_df, stories_df, how='right', on=None, left_on='key', right_on='Epic Link',\n",
    "         left_index=False, right_index=False, sort=True,\n",
    "         suffixes=('_epic', '_story'), copy=True, indicator=False,\n",
    "         validate=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the sprints with the epics + stories dataframe and we can then drop the duplicate issue_key field.\n",
    "\n",
    "sprintsWithStoriesAndEpics_df = pd.merge(scope_df, sprints_df, how='left', on=None, left_on='key_story', right_on='issue_key',\n",
    "         left_index=False, right_index=False, \n",
    "         suffixes=('_story', '_sprint'),\n",
    "         copy=True, indicator=True,\n",
    "         validate=None).drop(columns = ['issue_key'])\n",
    "\n",
    "#We can drop stories that are in future sprints\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['state'] != 'FUTURE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Lets make a list of problematic stories - \n",
    "#stories which are not in any sprints yet\n",
    "#stories which are in inactive sprints\n",
    "#after that we will elimiate these stories\n",
    "print('Stories that are not yet unassigned to sprints: ')\n",
    "print() \n",
    "sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['_merge'] == 'left_only'][['key_story', 'reporter_story', 'summary_story', 'status_story']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminiate the stories that are not assigned to sprints.\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['_merge'] != 'left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the stories which were opened more than a day later than the sprint started \n",
    "#or those were inserted after the development sprints were over\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['Team_story', 'startDate', 'state', 'Open Set To Date', 'reporter_story', 'Story Points', 'key_story', 'name', 'fixVersions_story']].copy()\n",
    "\n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] = (sprintsWithStoriesAndEpics_dfCopy['Open Set To Date'] - sprintsWithStoriesAndEpics_dfCopy['startDate']).dt.days \n",
    "sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] = sprintsWithStoriesAndEpics_dfCopy['sprintLeadTime'] <= 1\n",
    "#sprintsWithStoriesAndEpics_dfCopy['key_story'].unique().size\n",
    "sprintsWithStoriesAndEpics_dfCopy['beyondReleaseDeadline'] = sprintsWithStoriesAndEpics_dfCopy['Open Set To Date'] >= releaseStoryDeadline\n",
    "\n",
    "df = sprintsWithStoriesAndEpics_dfCopy[(sprintsWithStoriesAndEpics_dfCopy['sprintCommitment'] != True)|(sprintsWithStoriesAndEpics_dfCopy['beyondReleaseDeadline'] == True)].sort_values(by='key_story')\n",
    "#df = sprintsWithStoriesAndEpics_dfCopy\n",
    "#df = df[df['state'] == 'ACTIVE']\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "df.to_excel(writer, index=False, sheet_name='Late Commitments', freeze_panes=(1,0), columns=['Team_story', 'startDate', 'Open Set To Date', 'reporter_story', 'Story Points', 'key_story', 'name', 'sprintLeadTime', 'sprintCommitment'])\n",
    "df.sort_values('key_story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove the stories which we do not care about - closed\n",
    "stories_df = stories_df[stories_df['status'] != \"Closed\"]\n",
    "scope_df = scope_df[scope_df['status_story'] != \"Closed\"]\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['status_story'] != \"Closed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the latest sprint that the stories are in and then we can filter the ones that sprints that are closed.\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df.loc[sprintsWithStoriesAndEpics_df.groupby(\"key_story\")[\"startDate\"].idxmax()]\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[pd.notnull(sprintsWithStoriesAndEpics_df.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stories that have the most recent sprint inactive ')\n",
    "\n",
    "#next step is to print the ones out that have inactive sprints and eliminate them\n",
    "sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['state'] == 'CLOSED'][['key_story', 'Epic Name', 'summary_story', 'status_story', 'name', 'startDate', 'state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminate the stories with recent inactive sprints\n",
    "sprintsWithStoriesAndEpics_df = sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['state'] != 'CLOSED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate sprint Age.\n",
    "#sprintsWithStoriesAndEpics_df['key_story'][165]\n",
    "#There are two scenarios we have not considered - if the sprint is not active anymore, the age should be zero\n",
    "#We can remove stories in inactive sprints and report them as having no sprints!\n",
    "#The second case if the last status change happened earlier than sprint start date.\n",
    "sprintsWithStoriesAndEpics_df['Sprint Age In Days'] = sprintsWithStoriesAndEpics_df.apply(lambda x: (now - max(x[x['status_story'] + ' Set To Date'], x['startDate'])).days, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List the stories with their status, age and sprint age.\n",
    "sprintsWithStoriesAndEpics_df[['key_story', 'Team_story', 'fixVersions_story', 'summary_story', 'status_story', 'Age In Days', 'Sprint Age In Days', 'Open Set To Date']].sort_values(by=['status_story'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<h1> Calculate the Stories not having any mention of AC or Acceptance.</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a list of strings\n",
    "#scope_df['textinfo'] = scope_df['textinfo_story'] + scope_df['textinfo_epic']\n",
    "scope_df['textinfo'] = scope_df['textinfo_story']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_df['Invalid AC'] = scope_df['textinfo'].str.contains('Acceptance|AC', case = False, regex = True) == False\n",
    "\n",
    "#write out the source data onto disk\n",
    "#however we want to write only the records which are duplicates. Better idea to remove the non duplicates.\n",
    "scope_df[scope_df['Invalid AC']].to_excel(writer, index=False, sheet_name='Invalid AC', freeze_panes=(1,0), columns=['Team_story', 'key_story', 'reporter_story'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_ac_df = scope_df[['reporter_story', 'Invalid AC']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce statistics for valid/invalid AC\n",
    "invalid_ac_df.groupby(['reporter_story']).sum().sort_values(by=['Invalid AC'], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Amongst the stories assigned to sprints, lets do the same analysis as above\n",
    "#A story may be part of many sprints. In each case we need to print the minimums, so need to construct a new dataframe\n",
    "\n",
    "#def ListSprintAges(df, status, threshold = 1, suffix = ''):\n",
    "#    print(status, \":\")\n",
    "#    print()\n",
    "#    df.apply(lambda x: \n",
    "#             print (x['key'+suffix], x['Sprint Age In Days']) if ((x['status'+suffix] == status) \n",
    "#                                                    and x['Sprint Age In Days'] and (x['Sprint Age In Days'] >= threshold)) else None, axis=1)\n",
    "#    print()\n",
    "\n",
    "#sprintsWithStoriesAndEpics_dfCopy = sprintsWithStoriesAndEpics_df[['key_story', 'status_story', 'Sprint Age In Days']].copy()\n",
    "#sprintsWithStoriesAndEpics_dfCopy.groupby(['key_story', 'status_story' ]).agg({'min'})\n",
    "#ListSprintAges(sprintsWithStoriesAndEpics_dfCopy, \"Open\", 3, '_story')\n",
    "#ListSprintAges(sprintsWithStoriesAndEpics_dfCopy, \"Approval\", 3, '_story')\n",
    "#ListSprintAges(sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['_merge'] == 'both'], \"In Analysis\", 1, '_story')\n",
    "#ListSprintAges(sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['_merge'] == 'both'], \"Ready for Estimation\", 1, '_story')\n",
    "#ListSprintAges(sprintsWithStoriesAndEpics_df[sprintsWithStoriesAndEpics_df['_merge'] == 'both'], \"In UI/UX\", 1, '_story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For stories in each of these states, we need to determine the age\n",
    "#def ListAges(df, status, threshold = 1, suffix = ''):\n",
    "#    print(status, \":\")\n",
    "#    print()\n",
    "#    df.apply(lambda x: \n",
    "#             print (x['key'+suffix], x['Age In Days']) if ((x['status'+suffix] == status) \n",
    "#                                                    and x['Age In Days'] and (x['Age In Days'] >= threshold)) else None, axis=1)\n",
    "#    print()\n",
    "    \n",
    "#ListAges(stories_df, \"Approval\", 1)\n",
    "#ListAges(stories_df, \"In Analysis\", 5)\n",
    "#ListAges(stories_df, \"Ready for Estimation\", 5)\n",
    "#ListAges(stories_df, \"In UI/UX\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
